{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dropout, Flatten, Activation, MaxPooling2D, Dense\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "import os, glob\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_path = '/kaggle/input/load-model/my_best_center_crop_resnet152v2_model.h5'\n",
    "model = load_model(model_path)\n",
    "name = 'centercrop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir\n",
    "test_dir='/kaggle/input/early-anthracnose-for-test/early_anthracnose'\n",
    "\n",
    "# test_ds\n",
    "ds_health =glob.glob('/kaggle/input/early-anthracnose-for-test/early_anthracnose/healthy/*') \n",
    "ds_anth =glob.glob('/kaggle/input/early-anthracnose-for-test/early_anthracnose/anthracnose/*')\n",
    "\n",
    "# test_df\n",
    "test_path = ds_health + ds_anth\n",
    "test_label = [\"healthy\" for _ in range(len(ds_health))] + [\"anthracnose\" for _ in range(len(ds_anth))]\n",
    "test_df = pd.DataFrame({\"path\":test_path, \"label\":test_label})\n",
    "\n",
    "# Define data preprocessing and data_generator\n",
    "\n",
    "def create_data_generators(test_df, image_size, batch_size):\n",
    "\n",
    "    # Define a function to crop the image\n",
    "    def crop_image(img):\n",
    "        # get the shape of the input image\n",
    "        h, w, _ = img.shape\n",
    "        \n",
    "\n",
    "        # set the desired crop size\n",
    "        crop_size = 224\n",
    "\n",
    "        # calculate the starting point of the crop\n",
    "        start_y = int((h - crop_size) / 2)\n",
    "        start_x = int((w - crop_size) / 2)\n",
    "\n",
    "        # crop the image\n",
    "        crop_img = img[start_y:start_y + crop_size, start_x:start_x + crop_size, :]\n",
    "\n",
    "        return crop_img\n",
    "\n",
    "   # Define the ImageDataGenerator with only rescaling for test data\n",
    "    test_generator = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=20,\n",
    "        rescale=1/255.0,\n",
    "        preprocessing_function=crop_image)\n",
    "\n",
    "   # Flow from DataFrame using test_generator for test data\n",
    "    test_generator_iterator = test_generator.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='path',\n",
    "        y_col='label',\n",
    "        target_size=(image_size, image_size),\n",
    "        class_mode=\"categorical\",\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "    test_batch, test_label_batch = next(test_generator_iterator)\n",
    "\n",
    "    return test_generator_iterator,test_batch\n",
    "\n",
    "\n",
    "# Call the function to create data generators\n",
    "test_gen_iter,test_batch  = create_data_generators(test_df=test_df,image_size=224, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0, accuracy0 = model.evaluate(test_gen_iter)\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder to the label column\n",
    "le.fit(test_df['label'])\n",
    "\n",
    "# Transform the label column to encoded labels\n",
    "true_labels = le.transform(test_df['label'])\n",
    "\n",
    "# test_df['label']와 predicted_labels 비교\n",
    "predicted_labels = model.predict(test_gen_iter)\n",
    "predicted_labels = np.argmax(predicted_labels, axis=1)  # 모델의 출력을 가장 높은 확률을 가진 클래스로 변환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def generate_classification_report(model,name):   \n",
    "    \n",
    "    with open(name+'_report.csv', 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for line in report.split('\\n'):\n",
    "            writer.writerow(line.split(','))\n",
    "        \n",
    "        evaluation_results = model.evaluate(test_gen_iter)\n",
    "        evaluation_results_header = ['Metric', 'Value']\n",
    "            \n",
    "        writer.writerow(['Loss', evaluation_results[0]])\n",
    "        writer.writerow(['Accuracy', evaluation_results[1]])\n",
    "        # 정확도(accuracy)를 계산합니다.\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        writer.writerow(['Accuracy_score', accuracy])\n",
    "\n",
    "        # 정밀도(precision)를 계산합니다.\n",
    "        precision = precision_score(true_labels, predicted_labels,average='micro')\n",
    "        writer.writerow(['Precision', precision])\n",
    "\n",
    "        # 재현율(recall)를 계산합니다.\n",
    "        recall = recall_score(true_labels, predicted_labels,average='micro')\n",
    "        writer.writerow(['Recall', recall])\n",
    "\n",
    "        # F1-스코어(F1-score)를 계산합니다.\n",
    "        f1 = f1_score(true_labels, predicted_labels,average='micro')\n",
    "        writer.writerow(['F1_score', f1])\n",
    "\n",
    "        # 결과를 출력합니다.\n",
    "        print(\"정확도: {:.4f}\".format(accuracy))\n",
    "        print(\"정밀도: {:.4f}\".format(precision))\n",
    "        print(\"재현율: {:.4f}\".format(recall))\n",
    "        print(\"F1-스코어: {:.4f}\".format(f1))\n",
    "        \n",
    "generate_classification_report(model=model,name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define confusion matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def generate_confusion_matrix(model,name):\n",
    "    # create a LabelBinarizer object\n",
    "   \n",
    "\n",
    "    # Create a LabelEncoder object\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Fit the label encoder to the label column\n",
    "    le.fit(test_df['label'])\n",
    "\n",
    "    # Transform the label column to encoded labels\n",
    "    true_labels = le.transform(test_df['label'])\n",
    "\n",
    "    # Transform the label column to binary list\n",
    "    label_list = le.transform(test_df['label']).tolist()\n",
    "\n",
    "    # Flatten the list of ground truth labels and convert to numpy array\n",
    "    true_labels = np.ravel(label_list)\n",
    "    # Use the model to predict the classes of the test data\n",
    "    predicted_labels = model.predict(test_gen_iter)\n",
    "\n",
    "    # Convert predicted labels to integer format\n",
    "    predicted_labels = tf.argmax(predicted_labels, axis=1).numpy()\n",
    "\n",
    "    # Get confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(cm)\n",
    "    \n",
    "    class_names = list(test_gen_iter.class_indices.keys())\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.imshow(cm, cmap=plt.cm.GnBu, interpolation='nearest')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(class_names))-0.5\n",
    "    plt.xticks(tick_marks, class_names, rotation=20)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    plt.xlabel('Predicted Label', labelpad=25)\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    thresh = cm.max()-20\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i,j], \"d\"), ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i,j] > thresh else \"black\", fontsize=16)\n",
    "\n",
    "     # Save the figure as a JPG image\n",
    "    plt.savefig(name +'_confusion_matrix.jpg', dpi=500,bbox_inches='tight')\n",
    "generate_confusion_matrix(model=model,name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def extract_features(model, data):\n",
    "    # 주어진 모델의 입력과 'global_average_pooling2d' 레이어의 출력을 연결하는 모델, 이를 통해 모델의 특성을 추출\n",
    "    feature_extractor = Model(inputs=model.input, outputs=model.get_layer('flatten').output)\n",
    "    features = feature_extractor.predict(data)\n",
    "    return features\n",
    "\n",
    "# 모델과 데이터로부터 특성 추출\n",
    "features = extract_features(model,test_gen_iter)\n",
    "# TSNE를 사용하여 2차원으로 차원 축소\n",
    "tsne = TSNE(n_components=2,perplexity=5).fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블에 따라 색상 다르게\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# 시각화할 데이터 포인트의 색상과 레이블 설정\n",
    "colors_per_class = {\n",
    "    'anthracnose': 'red',\n",
    "    'healthy': 'blue'\n",
    "}\n",
    "\n",
    "# 시각화할 데이터 포인트의 색상 저장하는 리스트 초기화\n",
    "scatter_colors = []\n",
    "\n",
    "# test_df의 각 데이터 포인트에 대해 색상을 저장\n",
    "for _, data_point in test_df.iterrows():\n",
    "    color = colors_per_class[data_point['label']]\n",
    "    scatter_colors.append(color)\n",
    "\n",
    "# 산점도 그리기\n",
    "plt.scatter(tsne[:, 0], tsne[:, 1], c=scatter_colors)\n",
    "\n",
    "# 그래프 제목 설정\n",
    "plt.title(\"t-SNE Visualization\")\n",
    "\n",
    "# x축, y축 레이블 설정\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "# 범례 항목 생성\n",
    "legend_patches = [mpatches.Patch(color=color, label=label) for label, color in colors_per_class.items()]\n",
    "\n",
    "# 범례 표시\n",
    "plt.legend(handles=legend_patches)\n",
    "\n",
    "# 그래프 출력\n",
    "plt.savefig(name+'_t-sne.png',dpi=500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA를 사용하여 2차원으로 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(features)\n",
    "\n",
    "# 레이블에 따라 색상 다르게\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# 시각화할 데이터 포인트의 색상과 레이블 설정\n",
    "colors_per_class = {\n",
    "    'bacterial_spot': 'red',\n",
    "    'powdery_mildew' : 'green',\n",
    "    'PepMoV': 'purple',\n",
    "    'healthy': 'blue'\n",
    "}\n",
    "\n",
    "# 시각화할 데이터 포인트의 색상 저장하는 리스트 초기화\n",
    "scatter_colors = []\n",
    "\n",
    "# test_df의 각 데이터 포인트에 대해 색상을 저장\n",
    "for _, data_point in test_df.iterrows():\n",
    "    color = colors_per_class[data_point['label']]\n",
    "    scatter_colors.append(color)\n",
    "\n",
    "# 산점도 그리기\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=scatter_colors)\n",
    "\n",
    "# 그래프 제목 설정\n",
    "plt.title(\"PCA Visualization\")\n",
    "\n",
    "# x축, y축 레이블 설정\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "# 범례 항목 생성\n",
    "legend_patches = [mpatches.Patch(color=color, label=label) for label, color in colors_per_class.items()]\n",
    "\n",
    "# 범례 표시\n",
    "plt.legend(handles=legend_patches)\n",
    "\n",
    "# 그래프 출력\n",
    "plt.savefig(name+'_pca.png',dpi=500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def gradcam_visualization(model, image, layer_name, class_index):\n",
    "    # GradCAM을 위한 모델 생성\n",
    "    grad_model = Model(inputs=model.input, outputs=(model.get_layer(layer_name).output, model.output))\n",
    "\n",
    "    # 이미지 전처리\n",
    "    img = np.expand_dims(image, axis=0)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # 특성 맵과 예측 결과 가져오기\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img)\n",
    "        loss = predictions[:, class_index]\n",
    "\n",
    "    # 클래스에 대한 gradient 계산\n",
    "    grads = tape.gradient(loss, conv_outputs)[0]\n",
    "\n",
    "    # 특성 맵과 gradient를 가중 평균하여 heatmap 생성\n",
    "    weights = tf.reduce_mean(grads, axis=(0, 1))\n",
    "    heatmap = tf.reduce_sum(tf.multiply(weights, conv_outputs[0]), axis=-1)\n",
    "\n",
    "    # heatmap 후처리\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    # heatmap을 원본 이미지 크기로 조정\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # heatmap을 원본 이미지에 적용하여 시각화\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = cv2.addWeighted(image, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    return superimposed_img\n",
    "\n",
    "# GradCAM 시각화 예시\n",
    "class_index = 0  # 시각화할 클래스 인덱스\n",
    "layer_name = 'global_average_pooling2d'  # 시각화할 레이어 이름\n",
    "image = test_batch[0]  # 시각화할 이미지\n",
    "\n",
    "visualization = gradcam_visualization(model, image, layer_name, class_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

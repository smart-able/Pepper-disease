{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smart-able/Pepper-disease/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe9vkEvFABbN"
      },
      "source": [
        "[![Roboflow Notebooks](https://ik.imagekit.io/roboflow/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Train YOLOv8 Classification on a Custom Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset)\n",
        "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wuZtUMEiKWY)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics)\n",
        "\n",
        "Ultralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.\n",
        "\n",
        "## ‚ö†Ô∏è Disclaimer\n",
        "\n",
        "As of 18.01.2023, YOLOv8 Classification seems a tad underdeveloped. It is possible to train models, but their usability is questionable. Known problems include:\n",
        "- The model pre-trained on the Imagenet dataset operates on the id of classes not their names. Only after custom post-processing can you find out how the image was classified. \n",
        "- No detailed training data available. At this point, it is almost standard to save information such as confusion matrix or graphs of key metrics after a training session is completed. YOLOv8 offers this feature but for the moment only for Object Detection and Instance Segmentation.\n",
        "- In the case of the CLI, there is no saving of the prediction in text form. They are annotated on the image, which in practice makes it impossible to build any application using them.\n",
        "- With SDK, a difficult-to-interpret matrix is returned instead of a vector of probabilities. Only after custom post-processing can you find out how the image was classified. \n",
        "\n",
        "## Accompanying Blog Post\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on how to train YOLOv8 Classification, concurrently.\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Before you start\n",
        "- Install YOLOv8\n",
        "- CLI Basics\n",
        "- Inference with Pre-trained COCO Model\n",
        "- Roboflow Universe\n",
        "- Preparing a custom dataset\n",
        "- Custom Training\n",
        "- Validate Custom Model\n",
        "- Inference with Custom Model\n",
        "\n",
        "**Let's begin!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyRdDYkqAKN4"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8cDtxLIBHgQ",
        "outputId": "787e983c-15b1-40ca-f195-da92539dd9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 16 09:13:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0    17W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjpPg4mGKc1v",
        "outputId": "62471967-0528-47f8-b578-9381824bb622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C3EO_2zNChu"
      },
      "source": [
        "## Install YOLOv8\n",
        "\n",
        "‚ö†Ô∏è YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **18.01.2023** with version **YOLOv8.0.9**.\n",
        "\n",
        "If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n",
        "\n",
        "YOLOv8 can be installed in two ways‚Ää-‚Ääfrom the source and via pip. This is because it is the first iteration of YOLO to have an official package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdSMcABDNKW-",
        "outputId": "e7edad10-d9db-44f9-e2dd-8d42a40cf59e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.53 üöÄ Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 25.5/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVvaIYEEPOty"
      },
      "outputs": [],
      "source": [
        "# Git clone method (for development)\n",
        "\n",
        "# %cd {HOME}\n",
        "# !git clone github.com/ultralytics/ultralytics\n",
        "# %cd {HOME}/ultralytics\n",
        "# !pip install -qe ultralytics\n",
        "\n",
        "# from IPython import display\n",
        "# display.clear_output()\n",
        "\n",
        "# import ultralytics\n",
        "# ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VOEYrlBoP9-E"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display, Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnnZSm5OQfPQ"
      },
      "source": [
        "## CLI Basics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K33S7zlkQku0"
      },
      "source": [
        "If you want to train, validate or run inference on models and don't need to make any modifications to the code, using YOLO command line interface is the easiest way to get started. Read more about CLI in [Ultralytics YOLO Docs](https://v8docs.ultralytics.com/cli/).\n",
        "\n",
        "```\n",
        "yolo task=detect    mode=train    model=yolov8n.yaml      args...\n",
        "          classify       predict        yolov8n-cls.yaml  args...\n",
        "          segment        val            yolov8n-seg.yaml  args...\n",
        "                         export         yolov8n.pt        format=onnx  args...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5RGYA6sPgEd"
      },
      "source": [
        "## Inference with Pre-trained COCO Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT1qD4toTTw0"
      },
      "source": [
        "### üíª CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaE1kLS8R4CV"
      },
      "source": [
        "`yolo mode=predict` runs YOLOv8 inference on a variety of sources, downloading models automatically from the latest YOLOv8 release, and saving results to `runs/predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDbMt_M6PiXb",
        "outputId": "f9492a53-9bef-45e1-8040-b63fbb2e1f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-cls.pt to yolov8n-cls.pt...\n",
            "100% 5.28M/5.28M [00:00<00:00, 7.62MB/s]\n",
            "Ultralytics YOLOv8.0.53 üöÄ Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 2715880 parameters, 0 gradients, 4.3 GFLOPs\n",
            "\n",
            "Downloading https://media.roboflow.com/notebooks/examples/dog.jpeg to dog.jpeg...\n",
            "100% 104k/104k [00:00<00:00, 43.8MB/s]\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 86.9MB/s]\n",
            "image 1/1 /content/dog.jpeg: 224x224 basset 0.46, beagle 0.27, Walker_hound 0.20, English_foxhound 0.02, bloodhound 0.01, 6.0ms\n",
            "Speed: 0.4ms preprocess, 6.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!yolo task=classify mode=predict model=yolov8n-cls.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "LyopYpK1TQrB",
        "outputId": "68fe9e7f-7e0c-4483-8388-214a3625738d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-efcd62d41a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{HOME}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs/classify/predict/dog.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1204\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/classify/predict/dog.jpeg'"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "Image(filename='runs/classify/predict/dog.jpeg', height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFMBYQtMVL-B"
      },
      "source": [
        "### üêç Python SDK\n",
        "\n",
        "The simplest way of simply using YOLOv8 directly in a Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx9NWF-sVN6Y",
        "outputId": "c26fd4d5-c7ac-471e-96dd-029a05dbf90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Found https://media.roboflow.com/notebooks/examples/dog.jpeg locally at dog.jpeg\n",
            "image 1/1 /content/dog.jpeg: 224x224 basset 0.46, beagle 0.27, Walker_hound 0.20, English_foxhound 0.02, bloodhound 0.01, 3.9ms\n",
            "Speed: 0.3ms preprocess, 3.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        }
      ],
      "source": [
        "model = YOLO(f'{HOME}/yolov8n-cls.pt')\n",
        "results = model.predict(source='https://media.roboflow.com/notebooks/examples/dog.jpeg', conf=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Xtaekw3271"
      },
      "source": [
        "## Roboflow Universe\n",
        "\n",
        "Need data for your project? Before spending time on annotating, check out Roboflow Universe, a repository of more than 110,000 open-source datasets that you can use in your projects. You'll find datasets containing everything from annotated cracks in concrete to plant images with disease annotations.\n",
        "\n",
        "\n",
        "[![Roboflow Universe](https://ik.imagekit.io/roboflow/notebooks/template/uni-banner-frame.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672878480290)](https://universe.roboflow.com/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JHICVjZbVKn"
      },
      "source": [
        "## Preparing a custom¬†dataset\n",
        "\n",
        "Building a custom dataset can be a painful process. It might take dozens or even hundreds of hours to collect images, label them, and export them in the proper format. Fortunately, Roboflow makes this process as straightforward and fast as possible. Let me show you how!\n",
        "\n",
        "### Step 1: Creating project\n",
        "\n",
        "Before you start, you need to create a Roboflow [account](https://app.roboflow.com/login). Once you do that, you can create a new project in the Roboflow [dashboard](https://app.roboflow.com/). Keep in mind to choose the right project type. In our case, Object Detection.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://ik.imagekit.io/roboflow/preparing-custom-dataset-example/creating-project.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929799852\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 2: Uploading images\n",
        "\n",
        "Next, add the data to your newly created project. You can do it via API or through our [web interface](https://docs.roboflow.com/adding-data/object-detection).\n",
        "\n",
        "If you drag and drop a directory with a dataset in a supported format, the Roboflow dashboard will automatically read the images and annotations together. \n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://ik.imagekit.io/roboflow/preparing-custom-dataset-example/uploading-images.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929808290\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 3: Labeling\n",
        "\n",
        "If you only have images, you can label them in [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://user-images.githubusercontent.com/26109316/210901980-04861efd-dfc0-4a01-9373-13a36b5e1df4.gif\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 4: Generate new dataset version\n",
        "\n",
        "Now that we have our images and annotations added, we can Generate a Dataset Version. When Generating a Version, you may elect to add preprocessing and augmentations. This step is completely optional, however, it can allow you to significantly improve the robustness of your model.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/generate-new-version.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1673003597834\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 5: Exporting dataset\n",
        "\n",
        "Once the dataset version is generated, we have a hosted dataset we can load directly into our notebook for easy training. Click `Export` and select the `YOLO v5 PyTorch` dataset format.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://ik.imagekit.io/roboflow/preparing-custom-dataset-example/export.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672943313709\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSd93ZJzZZKt",
        "outputId": "c8f701e0-4d8f-42d3-dc0d-248164b9f97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in pepper-1 to clip: 91% [38060032 / 41756071] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to pepper-1 in clip:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2281/2281 [00:00<00:00, 2366.18it/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "!pip install roboflow --quiet\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"OfXyhjv4iYCkh3XHHvg3\")\n",
        "project = rf.workspace(\"plant-virus\").project(\"pepper-qgkfn\")\n",
        "dataset = project.version(1).download(\"clip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUjFBKKqXa-u"
      },
      "source": [
        "## Custom Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2YkphuiaE7_",
        "outputId": "21ddac3e-3523-4f8f-fc05-6932928601b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics YOLOv8.0.53 üöÄ Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/datasets/pepper-1, epochs=25, patience=50, batch=16, imgsz=128, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/classify/train\n",
            "Overriding model.yaml nc=1000 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
            "  9                  -1  1    332802  ultralytics.nn.modules.Classify              [256, 2]                      \n",
            "YOLOv8n-cls summary: 99 layers, 1440850 parameters, 1440850 gradients, 3.4 GFLOPs\n",
            "2023-03-16 09:18:32.956736: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-16 09:18:33.999726: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-16 09:18:33.999833: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-16 09:18:33.999851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt...\n",
            "100% 6.23M/6.23M [00:00<00:00, 21.4MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=128, width=128, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n",
            "Image sizes 128 train, 128 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       1/25    0.0902G      0.165          2        128: 100% 100/100 [00:07<00:00, 13.92it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 47.78it/s]\n",
            "                   all      0.712          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       2/25     0.126G     0.1513          2        128: 100% 100/100 [00:05<00:00, 18.34it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 23.07it/s]\n",
            "                   all      0.803          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       3/25     0.126G     0.1122          2        128: 100% 100/100 [00:05<00:00, 17.20it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 65.18it/s]\n",
            "                   all      0.725          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       4/25     0.126G    0.08217          2        128: 100% 100/100 [00:04<00:00, 20.05it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 62.15it/s]\n",
            "                   all      0.795          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       5/25     0.126G    0.07596          2        128: 100% 100/100 [00:06<00:00, 16.05it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 35.00it/s]\n",
            "                   all      0.904          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       6/25     0.126G    0.06862          2        128: 100% 100/100 [00:04<00:00, 20.49it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 32.30it/s]\n",
            "                   all      0.904          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       7/25     0.126G    0.05874          2        128: 100% 100/100 [00:06<00:00, 15.99it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 63.23it/s]\n",
            "                   all      0.952          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       8/25     0.126G    0.05368          2        128: 100% 100/100 [00:05<00:00, 17.78it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 36.87it/s]\n",
            "                   all      0.943          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       9/25     0.126G    0.05744          2        128: 100% 100/100 [00:06<00:00, 15.88it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 40.20it/s]\n",
            "                   all      0.943          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      10/25     0.126G    0.05086          2        128: 100% 100/100 [00:04<00:00, 20.09it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 32.96it/s]\n",
            "                   all      0.934          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      11/25     0.126G    0.04627          2        128: 100% 100/100 [00:06<00:00, 15.56it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 41.15it/s]\n",
            "                   all      0.943          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      12/25     0.126G    0.04171          2        128: 100% 100/100 [00:04<00:00, 20.30it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 37.60it/s]\n",
            "                   all      0.965          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      13/25     0.126G     0.0411          2        128: 100% 100/100 [00:06<00:00, 16.49it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 24.50it/s]\n",
            "                   all      0.956          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      14/25     0.126G     0.0445          2        128: 100% 100/100 [00:05<00:00, 19.82it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 33.14it/s]\n",
            "                   all      0.956          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      15/25     0.126G    0.03825          2        128: 100% 100/100 [00:04<00:00, 20.73it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 27.81it/s]\n",
            "                   all      0.961          1\n",
            "Closing dataloader mosaic\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      16/25     0.126G    0.03421          2        128: 100% 100/100 [00:06<00:00, 16.33it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 32.85it/s]\n",
            "                   all      0.956          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      17/25     0.126G    0.03303          2        128: 100% 100/100 [00:04<00:00, 20.09it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 35.49it/s]\n",
            "                   all      0.978          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      18/25     0.126G    0.03209          2        128: 100% 100/100 [00:06<00:00, 15.58it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 32.77it/s]\n",
            "                   all      0.961          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      19/25     0.126G    0.03232          2        128: 100% 100/100 [00:04<00:00, 20.42it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 36.50it/s]\n",
            "                   all      0.965          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      20/25     0.126G    0.02874          2        128: 100% 100/100 [00:06<00:00, 15.98it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 40.81it/s]\n",
            "                   all      0.969          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      21/25     0.126G    0.03091          2        128: 100% 100/100 [00:05<00:00, 19.99it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 35.51it/s]\n",
            "                   all      0.965          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      22/25     0.126G    0.02722          2        128: 100% 100/100 [00:06<00:00, 15.73it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 35.15it/s]\n",
            "                   all      0.952          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      23/25     0.126G    0.02944          2        128: 100% 100/100 [00:05<00:00, 19.82it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 70.76it/s]\n",
            "                   all      0.965          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      24/25     0.126G    0.02946          2        128: 100% 100/100 [00:06<00:00, 15.85it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 30.46it/s]\n",
            "                   all      0.978          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      25/25     0.126G    0.02521          2        128: 100% 100/100 [00:04<00:00, 20.28it/s]\n",
            "               classes   top1_acc   top5_acc: 100% 4/4 [00:00<00:00, 48.15it/s]\n",
            "                   all      0.969          1\n",
            "\n",
            "25 epochs completed in 0.043 hours.\n",
            "Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "!yolo task=classify mode=train model=yolov8n-cls.pt data={dataset.location} epochs=25 imgsz=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MScstfHhArr",
        "outputId": "e8bc4e08-3a93-42f5-f7e7-ec854e3cd628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 148\n",
            "drwxr-xr-x 3 root root   4096 Mar 16 09:18 .\n",
            "drwxr-xr-x 3 root root   4096 Mar 16 09:18 ..\n",
            "-rw-r--r-- 1 root root   1373 Mar 16 09:18 args.yaml\n",
            "-rw-r--r-- 1 root root 124140 Mar 16 09:21 events.out.tfevents.1678958315.72f5f6bfe816.2231.0\n",
            "-rw-r--r-- 1 root root   4992 Mar 16 09:21 results.csv\n",
            "drwxr-xr-x 2 root root   4096 Mar 16 09:18 weights\n"
          ]
        }
      ],
      "source": [
        "!ls -la {HOME}/runs/classify/train/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat {HOME}/runs/classify/train/results.csv | head -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ZOpnH9scaU",
        "outputId": "766ec388-3bb7-4bbd-af4f-3b23c49e5c40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  epoch,             train/loss,  metrics/accuracy_top1,  metrics/accuracy_top5,               val/loss,                 lr/pg0,                 lr/pg1,                 lr/pg2\n",
            "                      0,                0.16503,                0.71179,                      1,                0.56616,                 0.0703,                 0.0033,                 0.0033\n",
            "                      1,                0.15135,                0.80349,                      1,                0.51312,               0.040037,              0.0063707,              0.0063707\n",
            "                      2,                0.11218,                0.72489,                      1,                0.48962,              0.0095106,              0.0091773,              0.0091773\n",
            "                      3,                0.08217,                0.79476,                      1,                0.42877,               0.008812,               0.008812,               0.008812\n",
            "                      4,                0.07596,                0.90393,                      1,                0.37054,               0.008812,               0.008812,               0.008812\n",
            "                      5,                0.06862,                0.90393,                      1,                0.36835,               0.008416,               0.008416,               0.008416\n",
            "                      6,                0.05874,                0.95197,                      1,                0.33041,                0.00802,                0.00802,                0.00802\n",
            "                      7,                0.05368,                0.94323,                      1,                0.33246,               0.007624,               0.007624,               0.007624\n",
            "                      8,                0.05744,                0.94323,                      1,                0.34698,               0.007228,               0.007228,               0.007228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ODk1VTlevxn"
      },
      "source": [
        "## Validate Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpyuwrNlXc1P",
        "outputId": "2f28b8e1-330f-4dca-ab2e-87d0dedbd510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics YOLOv8.0.53 üöÄ Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1437442 parameters, 0 gradients, 3.3 GFLOPs\n",
            "               classes   top1_acc   top5_acc: 100% 15/15 [00:00<00:00, 37.16it/s]\n",
            "                   all      0.969          1\n",
            "Speed: 0.0ms preprocess, 0.5ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/val2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "!yolo task=classify mode=val model={HOME}/runs/classify/train/weights/best.pt data={dataset.location}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eASbcWkQBq"
      },
      "source": [
        "## Inference with Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjc1ctZykYuf",
        "outputId": "266bef4b-fe01-45d7-8963-bd4d87ab252e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics YOLOv8.0.53 üöÄ Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1437442 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\n",
            "image 1/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a01_20201102_0002_S01_1_jpg.rf.4b0df02a2fbbece042b8a864d7119f76.jpg: 128x128 healthy 0.99, anthracnose 0.01, 4.7ms\n",
            "image 2/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0005_S01_1_JPG_jpg.rf.894cf9e7dcf0c9aefa3cba861d85cb7c.jpg: 128x128 healthy 1.00, anthracnose 0.00, 4.4ms\n",
            "image 3/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0007_S01_1_JPG_jpg.rf.840af82d18ce24099444724b5aeee3d7.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 4/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0029_S01_1_JPG_jpg.rf.5b31245d141ea1c3ac00480071e7d942.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 5/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0035_S01_1_JPG_jpg.rf.13861663b1ca1f444b19df27b666a2b5.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 6/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0043_S01_1_JPG_jpg.rf.7e7e17072c5502cadd4e045d3d7bb7cb.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 7/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0072_S01_1_JPG_jpg.rf.c073191db72bf12dc906cae72b42b4d2.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.4ms\n",
            "image 8/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0073_S01_1_JPG_jpg.rf.1a076102fb71bfc987aaace58ba93989.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 9/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0077_S01_1_JPG_jpg.rf.e471ae389a9f6d5b7a3726d4fa3f5bef.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 10/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0080_S01_1_JPG_jpg.rf.4adeaf7a258ab7f0c803bc3743996fa4.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 11/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0082_S01_1_JPG_jpg.rf.656f6b85504044e988144ee5b6f35c63.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 12/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0083_S01_1_JPG_jpg.rf.f2b1198b8c4abb14466a53ab19af8fb7.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.4ms\n",
            "image 13/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0085_S01_1_JPG_jpg.rf.e464c62e296311468b579439999e18d4.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 14/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0098_S01_1_JPG_jpg.rf.0d706daf8560e404f6c5c7bac181a9ad.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.1ms\n",
            "image 15/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0105_S01_1_JPG_jpg.rf.36dc3b95bd6f530f711887cbb2fe7aaf.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 16/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0108_S01_1_JPG_jpg.rf.a97b12c5f53cfa29ad6e72e659a6bc21.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.4ms\n",
            "image 17/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0118_S01_1_JPG_jpg.rf.9a6902c9c05d8efc46ae38203b9423db.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 18/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0121_S01_1_JPG_jpg.rf.8808f6cde65a10b4febb4bb6a88f6762.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 19/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0138_S01_1_JPG_jpg.rf.39a2224f65af0c2468fb1c30db59a997.jpg: 128x128 healthy 0.99, anthracnose 0.01, 4.8ms\n",
            "image 20/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0141_S01_1_JPG_jpg.rf.cf80f2d1aa7c8af1fff424aeec035a44.jpg: 128x128 healthy 0.99, anthracnose 0.01, 5.1ms\n",
            "image 21/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a05_20201111_0144_S01_1_JPG_jpg.rf.084143412297b559fbec9e8d668e06ef.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.9ms\n",
            "image 22/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_a06_20201027_0003_S01_1_jpg.rf.eeaefea36c7f5ee8665ce75418055a80.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.4ms\n",
            "image 23/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b06_20201007_0002_S01_1_jpg.rf.66c3481e4739fc4fd93b914414104f89.jpg: 128x128 healthy 0.82, anthracnose 0.18, 3.1ms\n",
            "image 24/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b06_20201019_0001_S01_1_jpg.rf.1e1501db78e272434d015551a6ce8308.jpg: 128x128 healthy 0.88, anthracnose 0.12, 3.0ms\n",
            "image 25/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b06_20201029_0007_S01_1_jpg.rf.48c0697d327204652bd26b1ba20a191a.jpg: 128x128 anthracnose 0.82, healthy 0.18, 3.3ms\n",
            "image 26/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b07_20201109_0000_S01_1_jpg.rf.28accb73dce4a55d14be12f46fc28828.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 27/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b08_20201022_0011_S01_1_jpg.rf.306f223bf71700dc3f404ba6a61fb29b.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 28/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b09_20201007_0002_S01_1_jpg.rf.cb3417eb6398c162d50d3459cd34eed4.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.0ms\n",
            "image 29/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b09_20201007_0019_S01_1_jpg.rf.bf8f6be9ba240e1879af234f2005e345.jpg: 128x128 healthy 0.98, anthracnose 0.02, 3.0ms\n",
            "image 30/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b09_20201007_0021_S01_1_jpg.rf.551add2aa5ac5b0daedf949323c58e38.jpg: 128x128 healthy 0.85, anthracnose 0.15, 3.1ms\n",
            "image 31/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b09_20201007_0024_S01_1_jpg.rf.856e9fa25f820059e0205070be7e4f12.jpg: 128x128 healthy 0.97, anthracnose 0.03, 3.0ms\n",
            "image 32/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b09_20201007_0028_S01_1_jpg.rf.55ce05dbfdfdb1ba7e5f8e4213a004f2.jpg: 128x128 healthy 0.93, anthracnose 0.07, 3.3ms\n",
            "image 33/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b10_20201127_0003_S01_1_jpg.rf.414d2fe3b63013727d7ed91c64c94945.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 34/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b10_20201127_0011_S01_1_jpg.rf.16536d262b27b7e51045c3eb16c01f7c.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 35/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b10_20201127_0013_S01_1_jpg.rf.6e6c40e6e09d3dfc599a9b91828a4b04.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 36/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_b10_20201127_0018_S01_1_jpg.rf.e81467ce328ef6847ef5843dfd624f90.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 37/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0033_S01_1_JPEG_jpg.rf.38e3c9276da7fc31e9fe2339a7691bdf.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 38/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0036_S01_1_JPEG_jpg.rf.0fcc0063c8344e00e951d9eb51ef2dab.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 39/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0042_S01_1_JPEG_jpg.rf.cf9f0319a88fb71d70659daf5b073807.jpg: 128x128 healthy 0.92, anthracnose 0.08, 3.4ms\n",
            "image 40/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0052_S01_1_JPEG_jpg.rf.9fe481211eeefbd5d75847f0c708844f.jpg: 128x128 healthy 1.00, anthracnose 0.00, 5.2ms\n",
            "image 41/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0068_S01_1_JPEG_jpg.rf.992d8f4bef7fad383c34ac1413aaf2fb.jpg: 128x128 healthy 0.98, anthracnose 0.02, 3.4ms\n",
            "image 42/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0071_S01_1_JPEG_jpg.rf.28573ba3feffe4b0182948fd84e52311.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 43/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0106_S01_1_JPEG_jpg.rf.70d63c0d6e1141aec63172ea4b0e6160.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 44/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0111_S01_1_JPEG_jpg.rf.fb23e94c649954750e4f424e3b2d0126.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 45/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0112_S01_1_JPEG_jpg.rf.0c9199ffbdc3628d036900857804b9a8.jpg: 128x128 healthy 1.00, anthracnose 0.00, 5.0ms\n",
            "image 46/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0117_S01_1_JPEG_jpg.rf.1a0113f8481ec3f0ea971ff14da07396.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.4ms\n",
            "image 47/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0123_S01_1_JPEG_jpg.rf.e31744ff9c9f232af7e0cddf12813677.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.4ms\n",
            "image 48/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0124_S01_1_JPEG_jpg.rf.654cb47109378731ef68483843f6e67f.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 49/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c19_20201109_0126_S01_1_JPEG_jpg.rf.fa7cadec7a9cc33efbdb872c430603bc.jpg: 128x128 healthy 0.98, anthracnose 0.02, 3.2ms\n",
            "image 50/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c20_20201109_0026_S01_1_jpg.rf.bbb3b82aed9bff39a69a49ee0e32979d.jpg: 128x128 healthy 1.00, anthracnose 0.00, 5.5ms\n",
            "image 51/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c20_20201109_0035_S01_1_jpg.rf.58008e5f78642f807314c6e44bcf55dc.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.4ms\n",
            "image 52/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c20_20201109_0074_S01_1_jpg.rf.04b9fc91807f54d3dc39a6c1ed86b717.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 53/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c20_20201109_0089_S01_1_jpg.rf.135dd51591cec72dff95217448c9dc0a.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 54/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c20_20201109_0097_S01_1_jpg.rf.fb9d7fab61f03530ebb3b2d8199ff01b.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.4ms\n",
            "image 55/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c20_20201109_0101_S01_1_jpg.rf.7ef179d04dced531ecf7fb9ac7e32c53.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.8ms\n",
            "image 56/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c25_20201026_0013_S01_1_jpg.rf.e31b606d1af9e6f01954cc85e6f074c0.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.3ms\n",
            "image 57/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c25_20201026_0014_S01_1_jpg.rf.e3c86616b74f1b03e5dfc9241a573bc4.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.4ms\n",
            "image 58/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c25_20201026_0015_S01_1_jpg.rf.121fc2f7be3782078b8cfee99b86222d.jpg: 128x128 healthy 0.98, anthracnose 0.02, 3.3ms\n",
            "image 59/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c25_20201026_0016_S01_1_jpg.rf.e96093b7decd4e6b61efab07cdadc997.jpg: 128x128 healthy 0.98, anthracnose 0.02, 3.3ms\n",
            "image 60/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c25_20201026_0020_S01_1_jpg.rf.9b52b354d9cfaae8a8a9f32654e483e6.jpg: 128x128 healthy 0.95, anthracnose 0.05, 3.3ms\n",
            "image 61/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c25_20201110_0016_S01_1_jpg.rf.99919db9d657e0c8935287fc2e4ce5f6.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.2ms\n",
            "image 62/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c25_20201110_0017_S01_1_jpg.rf.6c0a84b4ea3be57a0610785b1ab45fd4.jpg: 128x128 healthy 0.97, anthracnose 0.03, 3.5ms\n",
            "image 63/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c25_20201110_0021_S01_1_jpg.rf.b1136299b9620ae705432d375761bf30.jpg: 128x128 healthy 0.97, anthracnose 0.03, 3.3ms\n",
            "image 64/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201201_0005_S01_1_JPG_jpg.rf.415f9328d17201c923778d8cbac385b1.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.5ms\n",
            "image 65/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201202_0004_S01_1_JPG_jpg.rf.cba78f287925da1811f40ad0305f6eb8.jpg: 128x128 healthy 0.98, anthracnose 0.02, 3.3ms\n",
            "image 66/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201202_0006_S01_1_JPG_jpg.rf.8fe4615648f60cf7bf284e75857e5aa9.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 67/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201202_0009_S01_4_JPG_jpg.rf.7a8aaf2288ccd0825c1c31aaabbac687.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.5ms\n",
            "image 68/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201202_0027_S01_1_JPG_jpg.rf.6c1ee3f270ead01e4f9774f4c887a45d.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 69/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201202_0054_S01_4_JPG_jpg.rf.5c10d4ecb7535670242cf175c26388fe.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.2ms\n",
            "image 70/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201202_0055_S01_4_JPG_jpg.rf.5dad51ad7d561820e1ba47509af59b5c.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 71/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201202_0063_S01_4_JPG_jpg.rf.ace990d1aace041f038504942fa85119.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 72/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201203_0014_S01_1_JPG_jpg.rf.73fb013c6ce42102449db3afc12b0816.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 73/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201203_0024_S01_4_JPG_jpg.rf.072f4ca558ff9068fe718058845cc636.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 74/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201203_0031_S01_4_JPG_jpg.rf.b17d0d6e294697b3c3e048a66e0a0a1f.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 75/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201203_0045_S01_4_JPG_jpg.rf.88c4f684c3b05249fd183b7fed286272.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.8ms\n",
            "image 76/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0000_S01_1_JPG_jpg.rf.56f2fe7971799cc828e86fccaf7ac5ec.jpg: 128x128 healthy 1.00, anthracnose 0.00, 2.9ms\n",
            "image 77/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0000_S01_4_JPG_jpg.rf.c86991de42c1923262bbed384b2c934d.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 78/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0011_S01_4_JPG_jpg.rf.2fd0f9eea008aaf3333eb334760b543a.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 79/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0016_S01_4_JPG_jpg.rf.480e880c0aabf08eb0ddc985654aa8e7.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 80/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0017_S01_4_JPG_jpg.rf.c3d5790dabd90752839c66782ce0f2fa.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 81/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0020_S01_1_JPG_jpg.rf.d4b2ba77b9c55298b7d36c952c387012.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.4ms\n",
            "image 82/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0021_S01_4_JPG_jpg.rf.1adca1d0ae2d767c84ef83bfe11237e1.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.4ms\n",
            "image 83/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0024_S01_1_JPG_jpg.rf.0eae94f71825693de83261f014dd9545.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 84/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0031_S01_1_JPG_jpg.rf.3a66537c2ab4987153f76833c966495e.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 85/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0036_S01_1_JPG_jpg.rf.1339c241363478987a8fc67156d393a1.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 86/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0043_S01_1_JPG_jpg.rf.23323abf6d85a41a9fb4f2f378fed9c7.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 87/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0050_S01_4_JPG_jpg.rf.7720c20522012564fcea53c2b171fa00.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 88/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201208_0074_S01_4_JPG_jpg.rf.c08a979378fe4389b51d82ac3a7ef16e.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 89/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201209_0000_S01_1_JPG_jpg.rf.7f1a7c8eb941e23ff3148fc6771b09a7.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.8ms\n",
            "image 90/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201209_0001_S01_1_JPG_jpg.rf.64721fc1e10274aa3a5919a4d5525646.jpg: 128x128 healthy 1.00, anthracnose 0.00, 4.1ms\n",
            "image 91/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201209_0012_S01_1_JPG_jpg.rf.2cb69315882ee7ed851b2107cd06b1d7.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 92/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201209_0014_S01_1_JPG_jpg.rf.f91c2053469ede7a3d049c0533be934a.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 93/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c27_20201209_0035_S01_4_JPG_jpg.rf.9b88a4654a2a762e48c626c085103404.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 94/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201201_0005_S01_1_JPG_jpg.rf.c08f5243b7436efd96c91fd41a91b38f.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 95/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201201_0034_S01_1_JPG_jpg.rf.e997f89b51892e952f81be60813a0854.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 96/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201201_0058_S01_1_JPG_jpg.rf.2335c5b5d48b47f4cad09af609f6d570.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 97/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201202_0009_S01_1_JPG_jpg.rf.d8841785fe56bf4c1b9efc27d7447916.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 98/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201202_0013_S01_1_JPG_jpg.rf.6eb97f07594f2a2eb6500166198c201b.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 99/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201202_0016_S01_4_JPG_jpg.rf.a543d3ba3dc4a3de1d3fe72d4e4c762d.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 100/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201202_0030_S01_1_JPG_jpg.rf.88ea88e42b801fa8b07dbd22f175f035.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 101/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201202_0033_S01_4_JPG_jpg.rf.07a8e66c0b7c338596a19814e103fc9b.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 102/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201203_0025_S01_1_JPG_jpg.rf.14aa89fd2e3342562111b343a6a90c7b.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 103/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0015_S01_1_JPG_jpg.rf.9fd8afcf1758f6e0ed1012623844e254.jpg: 128x128 healthy 1.00, anthracnose 0.00, 2.9ms\n",
            "image 104/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0027_S01_4_JPG_jpg.rf.75851b8cf6cb995e15c8c5320eaeeeef.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 105/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0033_S01_4_JPG_jpg.rf.246edf1dbdeb760f843f55dc3759289d.jpg: 128x128 healthy 1.00, anthracnose 0.00, 2.9ms\n",
            "image 106/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0037_S01_4_JPG_jpg.rf.a0b324e27921ee9de399c17ccd252d26.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 107/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0041_S01_1_JPG_jpg.rf.c6371ec71b473f14bdc4531119e528c5.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 108/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0045_S01_4_JPG_jpg.rf.6acf4c62d05b978a0a53cc3431e096a3.jpg: 128x128 healthy 1.00, anthracnose 0.00, 2.9ms\n",
            "image 109/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0049_S01_4_JPG_jpg.rf.5dde083060341b797f14aa7b4ed20730.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.2ms\n",
            "image 110/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0050_S01_1_JPG_jpg.rf.ac0a2309e52bef1cbf044589d27f6d35.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 111/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0060_S01_1_JPG_jpg.rf.0176b3fcf6faeae55afcc285601887a6.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 112/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0061_S01_1_JPG_jpg.rf.f51e83f89bb55c374b10ee3f5204672e.jpg: 128x128 healthy 1.00, anthracnose 0.00, 2.9ms\n",
            "image 113/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0066_S01_1_JPG_jpg.rf.9e772a0476a08de8110d40fc1fa68b62.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 114/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201208_0112_S01_1_JPG_jpg.rf.39db4379ed388fee480c371dfe977f2a.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 115/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201209_0001_S01_1_JPG_jpg.rf.f65ae2d6c5715b843354cfe44f1189c5.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 116/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201209_0003_S01_4_JPG_jpg.rf.804bd471ed6841385c497c89bd7f944b.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 117/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201209_0004_S01_1_JPG_jpg.rf.40658f64e7c4cabb621a5882be989885.jpg: 128x128 healthy 1.00, anthracnose 0.00, 2.9ms\n",
            "image 118/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201209_0004_S01_4_JPG_jpg.rf.82df353997a0ca63c39bdbf0abd81cd8.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 119/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201209_0006_S01_1_JPG_jpg.rf.12b8cd345e75ea19e5ba5a6c868d6c7d.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 120/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c28_20201209_0016_S01_4_JPG_jpg.rf.06b2757c57b140a736e27536adfc8d49.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.6ms\n",
            "image 121/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c29_20201208_0010_S01_1_jpg.rf.ba384617cd5ba4d89757ce25077b65e0.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.6ms\n",
            "image 122/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c29_20201208_0036_S01_1_jpg.rf.2464235143bcf03aeaa3fc1d2075bfd2.jpg: 128x128 healthy 1.00, anthracnose 0.00, 4.3ms\n",
            "image 123/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c29_20201208_0065_S01_1_jpg.rf.ceee63ef0ffbe12b70ccb4f7eda99645.jpg: 128x128 healthy 1.00, anthracnose 0.00, 5.0ms\n",
            "image 124/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c29_20201208_0068_S01_1_jpg.rf.9ae6830d9930c50706b984142cc200d0.jpg: 128x128 healthy 1.00, anthracnose 0.00, 4.0ms\n",
            "image 125/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c29_20201209_0067_S01_1_jpg.rf.fe0ceb2ea523a588cbd818dccb21c261.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 126/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c29_20201209_0078_S01_1_jpg.rf.f362023713f70dcaad287b464dd66e8c.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.3ms\n",
            "image 127/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c31_20201029_0008_S01_1_jpg.rf.c10646d77784854b38a6fd957247bdbd.jpg: 128x128 healthy 0.59, anthracnose 0.41, 3.0ms\n",
            "image 128/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c32_20201019_0013_S01_1_jpg.rf.0fc10e301b7cdd7a294cd29deda6892c.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.0ms\n",
            "image 129/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c32_20201029_0033_S01_1_jpg.rf.596333b5df193e82ddc3150a4e45047e.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.1ms\n",
            "image 130/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c32_20201029_0041_S01_1_jpg.rf.c5272574effc4cee81f8e7179ae042b0.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "image 131/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c35_20201022_0023S01_1_jpg.rf.b2509438c88c80e2af2b1f5af8479c1f.jpg: 128x128 healthy 0.99, anthracnose 0.01, 3.3ms\n",
            "image 132/132 /content/datasets/pepper-1/test/healthy/V006_79_0_00_01_01_13_0_c35_20201022_0052S01_1_jpg.rf.af9f87be0564278b59c8ebdcc0aee33e.jpg: 128x128 healthy 1.00, anthracnose 0.00, 3.0ms\n",
            "Speed: 0.1ms preprocess, 3.3ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 128)\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!yolo task=classify mode=predict model={HOME}/runs/classify/train/weights/best.pt conf=0.25 source={dataset.location}/test/healthy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jbVjEtPAkz3j"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for image_path in glob.glob(f'{HOME}/runs/classify/val2/test/healthy/*.jpg')[:3]:\n",
        "      display(Image(filename=image_path, width=600))\n",
        "      print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovQgOj_xSNDg"
      },
      "source": [
        "## üèÜ Congratulations\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}